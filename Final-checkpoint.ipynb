{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b354e63",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "#导入数据和库\n",
    "import featuretools as ft\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report,f1_score\n",
    "from data_solve import data_solve_train_valid,data_processing_nomal,data_processing_nomal_test,data_solve_RFECV_2,data_solve_RFECV_test\n",
    "from autogluon.tabular import TabularPredictor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from autogluon.core.metrics import make_scorer\n",
    "\n",
    "train_filename = r\"preprocess_train.csv\"\n",
    "train = data_solve_RFECV_2(train_filename,100)\n",
    "\n",
    "\n",
    "valid_filename = r\"validate_1000.csv\"\n",
    "valid = data_solve_RFECV_2(valid_filename,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ee328fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#简答分割数据\n",
    "Train = train.drop(['sample_id','label'],axis = 1)\n",
    "Label = train[['label']]\n",
    "Valid = valid.drop(['sample_id','label'],axis = 1)\n",
    "Label_Valid = valid[['label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cd5e8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义评分函数之一\n",
    "def MacroF1(report):\n",
    "    prec_avg = report['macro avg']['precision']\n",
    "    reca_avg = report['macro avg']['recall']\n",
    "    macro_F1 = (2 * prec_avg * reca_avg) / (prec_avg + reca_avg)\n",
    "    return macro_F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b622b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建实体集合es\n",
    "es = ft.EntitySet(id = 'sales')\n",
    "# 将所有列名转换为字符串\n",
    "Train.columns = Train.columns.astype(str)\n",
    "Valid.columns = Valid.columns.astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a1869ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\envs\\myenv\\lib\\site-packages\\featuretools\\entityset\\entityset.py:754: UserWarning: A Woodwork-initialized DataFrame was provided, so the following parameters were ignored: index\n",
      "  warnings.warn(\n",
      "D:\\anaconda3\\envs\\myenv\\lib\\site-packages\\featuretools\\synthesis\\deep_feature_synthesis.py:169: UserWarning: Only one dataframe in entityset, changing max_depth to 1 since deeper features cannot be created\n",
      "  warnings.warn(\n",
      "D:\\anaconda3\\envs\\myenv\\lib\\site-packages\\featuretools\\synthesis\\dfs.py:321: UnusedPrimitiveWarning: Some specified primitives were not used during DFS:\n",
      "  agg_primitives: ['mean', 'median', 'sum']\n",
      "This may be caused by a using a value of max_depth that is too small, not setting interesting values, or it may indicate no compatible columns for the primitive were found in the data. If the DFS call contained multiple instances of a primitive in the list above, none of them were used.\n",
      "  warnings.warn(warning_msg, UnusedPrimitiveWarning)\n",
      "State start\n",
      "  Scheduler at:     tcp://127.0.0.1:64204\n",
      "  dashboard at:  http://127.0.0.1:8787/status\n",
      "        Start Nanny at: 'tcp://127.0.0.1:64210'\n",
      "        Start Nanny at: 'tcp://127.0.0.1:64207'\n",
      "        Start Nanny at: 'tcp://127.0.0.1:64213'\n",
      "        Start Nanny at: 'tcp://127.0.0.1:64212'\n",
      "        Start Nanny at: 'tcp://127.0.0.1:64211'\n",
      "        Start Nanny at: 'tcp://127.0.0.1:64208'\n",
      "        Start Nanny at: 'tcp://127.0.0.1:64209'\n",
      "        Start Nanny at: 'tcp://127.0.0.1:64214'\n",
      "Register worker <WorkerState 'tcp://127.0.0.1:64254', name: 2, status: init, memory: 0, processing: 0>\n",
      "Starting worker compute stream, tcp://127.0.0.1:64254\n",
      "Starting established connection to tcp://127.0.0.1:64262\n",
      "Register worker <WorkerState 'tcp://127.0.0.1:64263', name: 0, status: init, memory: 0, processing: 0>\n",
      "Starting worker compute stream, tcp://127.0.0.1:64263\n",
      "Starting established connection to tcp://127.0.0.1:64266\n",
      "Register worker <WorkerState 'tcp://127.0.0.1:64253', name: 6, status: init, memory: 0, processing: 0>\n",
      "Starting worker compute stream, tcp://127.0.0.1:64253\n",
      "Starting established connection to tcp://127.0.0.1:64260\n",
      "Register worker <WorkerState 'tcp://127.0.0.1:64248', name: 3, status: init, memory: 0, processing: 0>\n",
      "Starting worker compute stream, tcp://127.0.0.1:64248\n",
      "Starting established connection to tcp://127.0.0.1:64250\n",
      "Register worker <WorkerState 'tcp://127.0.0.1:64252', name: 1, status: init, memory: 0, processing: 0>\n",
      "Starting worker compute stream, tcp://127.0.0.1:64252\n",
      "Starting established connection to tcp://127.0.0.1:64258\n",
      "Register worker <WorkerState 'tcp://127.0.0.1:64251', name: 5, status: init, memory: 0, processing: 0>\n",
      "Starting worker compute stream, tcp://127.0.0.1:64251\n",
      "Starting established connection to tcp://127.0.0.1:64256\n",
      "Register worker <WorkerState 'tcp://127.0.0.1:64264', name: 7, status: init, memory: 0, processing: 0>\n",
      "Starting worker compute stream, tcp://127.0.0.1:64264\n",
      "Starting established connection to tcp://127.0.0.1:64268\n",
      "Register worker <WorkerState 'tcp://127.0.0.1:64270', name: 4, status: init, memory: 0, processing: 0>\n",
      "Starting worker compute stream, tcp://127.0.0.1:64270\n",
      "Starting established connection to tcp://127.0.0.1:64272\n",
      "Receive client connection: Client-0856ebeb-2255-11ee-a69c-14857f4ff0a5\n",
      "Starting established connection to tcp://127.0.0.1:64274\n",
      "Worker tcp://127.0.0.1:64254 failed to acquire keys: {'bytes-25fe4cceb87095db1a2b1b891f896e50': ('tcp://127.0.0.1:64263',)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EntitySet scattered to 8 workers in 5 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Remove client Client-0856ebeb-2255-11ee-a69c-14857f4ff0a5\n",
      "Received 'close-stream' from tcp://127.0.0.1:64274; closing.\n",
      "Remove client Client-0856ebeb-2255-11ee-a69c-14857f4ff0a5\n",
      "Close client connection: Client-0856ebeb-2255-11ee-a69c-14857f4ff0a5\n",
      "Closing Nanny at 'tcp://127.0.0.1:64207'. Reason: nanny-close\n",
      "Nanny asking worker to close. Reason: nanny-close\n",
      "Closing Nanny at 'tcp://127.0.0.1:64208'. Reason: nanny-close\n",
      "Nanny asking worker to close. Reason: nanny-close\n",
      "Closing Nanny at 'tcp://127.0.0.1:64209'. Reason: nanny-close\n",
      "Nanny asking worker to close. Reason: nanny-close\n",
      "Closing Nanny at 'tcp://127.0.0.1:64210'. Reason: nanny-close\n",
      "Nanny asking worker to close. Reason: nanny-close\n",
      "Closing Nanny at 'tcp://127.0.0.1:64211'. Reason: nanny-close\n",
      "Nanny asking worker to close. Reason: nanny-close\n",
      "Closing Nanny at 'tcp://127.0.0.1:64212'. Reason: nanny-close\n",
      "Nanny asking worker to close. Reason: nanny-close\n",
      "Closing Nanny at 'tcp://127.0.0.1:64213'. Reason: nanny-close\n",
      "Nanny asking worker to close. Reason: nanny-close\n",
      "Closing Nanny at 'tcp://127.0.0.1:64214'. Reason: nanny-close\n",
      "Nanny asking worker to close. Reason: nanny-close\n",
      "Received 'close-stream' from tcp://127.0.0.1:64266; closing.\n",
      "Received 'close-stream' from tcp://127.0.0.1:64268; closing.\n",
      "Received 'close-stream' from tcp://127.0.0.1:64272; closing.\n",
      "Received 'close-stream' from tcp://127.0.0.1:64250; closing.\n",
      "Received 'close-stream' from tcp://127.0.0.1:64256; closing.\n",
      "Received 'close-stream' from tcp://127.0.0.1:64260; closing.\n",
      "Received 'close-stream' from tcp://127.0.0.1:64262; closing.\n",
      "Received 'close-stream' from tcp://127.0.0.1:64258; closing.\n",
      "Remove worker <WorkerState 'tcp://127.0.0.1:64263', name: 0, status: closing, memory: 0, processing: 0>\n",
      "Removing comms to tcp://127.0.0.1:64263\n",
      "Remove worker <WorkerState 'tcp://127.0.0.1:64264', name: 7, status: closing, memory: 0, processing: 0>\n",
      "Removing comms to tcp://127.0.0.1:64264\n",
      "Remove worker <WorkerState 'tcp://127.0.0.1:64270', name: 4, status: closing, memory: 0, processing: 0>\n",
      "Removing comms to tcp://127.0.0.1:64270\n",
      "Remove worker <WorkerState 'tcp://127.0.0.1:64248', name: 3, status: closing, memory: 0, processing: 0>\n",
      "Removing comms to tcp://127.0.0.1:64248\n",
      "Remove worker <WorkerState 'tcp://127.0.0.1:64251', name: 5, status: closing, memory: 0, processing: 0>\n",
      "Removing comms to tcp://127.0.0.1:64251\n",
      "Remove worker <WorkerState 'tcp://127.0.0.1:64253', name: 6, status: closing, memory: 0, processing: 0>\n",
      "Removing comms to tcp://127.0.0.1:64253\n",
      "Remove worker <WorkerState 'tcp://127.0.0.1:64254', name: 2, status: closing, memory: 1, processing: 0>\n",
      "Removing comms to tcp://127.0.0.1:64254\n",
      "Remove worker <WorkerState 'tcp://127.0.0.1:64252', name: 1, status: closing, memory: 0, processing: 0>\n",
      "Removing comms to tcp://127.0.0.1:64252\n",
      "Lost all workers\n",
      "Scheduler closing...\n",
      "Scheduler closing all comms\n",
      "D:\\anaconda3\\envs\\myenv\\lib\\site-packages\\featuretools\\entityset\\entityset.py:1910: UserWarning: index id not found in dataframe, creating new integer column\n",
      "  warnings.warn(\n",
      "D:\\anaconda3\\envs\\myenv\\lib\\site-packages\\featuretools\\synthesis\\dfs.py:321: UnusedPrimitiveWarning: Some specified primitives were not used during DFS:\n",
      "  agg_primitives: ['mean', 'median', 'sum']\n",
      "This may be caused by a using a value of max_depth that is too small, not setting interesting values, or it may indicate no compatible columns for the primitive were found in the data. If the DFS call contained multiple instances of a primitive in the list above, none of them were used.\n",
      "  warnings.warn(warning_msg, UnusedPrimitiveWarning)\n",
      "State start\n",
      "  Scheduler at:     tcp://127.0.0.1:64523\n",
      "  dashboard at:  http://127.0.0.1:8787/status\n",
      "        Start Nanny at: 'tcp://127.0.0.1:64529'\n",
      "        Start Nanny at: 'tcp://127.0.0.1:64528'\n",
      "        Start Nanny at: 'tcp://127.0.0.1:64532'\n",
      "        Start Nanny at: 'tcp://127.0.0.1:64533'\n",
      "        Start Nanny at: 'tcp://127.0.0.1:64531'\n",
      "        Start Nanny at: 'tcp://127.0.0.1:64534'\n",
      "        Start Nanny at: 'tcp://127.0.0.1:64535'\n",
      "        Start Nanny at: 'tcp://127.0.0.1:64530'\n",
      "Register worker <WorkerState 'tcp://127.0.0.1:64567', name: 6, status: init, memory: 0, processing: 0>\n",
      "Starting worker compute stream, tcp://127.0.0.1:64567\n",
      "Starting established connection to tcp://127.0.0.1:64569\n",
      "Register worker <WorkerState 'tcp://127.0.0.1:64564', name: 3, status: init, memory: 0, processing: 0>\n",
      "Starting worker compute stream, tcp://127.0.0.1:64564\n",
      "Starting established connection to tcp://127.0.0.1:64566\n",
      "Register worker <WorkerState 'tcp://127.0.0.1:64570', name: 4, status: init, memory: 0, processing: 0>\n",
      "Starting worker compute stream, tcp://127.0.0.1:64570\n",
      "Starting established connection to tcp://127.0.0.1:64576\n",
      "Register worker <WorkerState 'tcp://127.0.0.1:64572', name: 7, status: init, memory: 0, processing: 0>\n",
      "Starting worker compute stream, tcp://127.0.0.1:64572\n",
      "Starting established connection to tcp://127.0.0.1:64578\n",
      "Register worker <WorkerState 'tcp://127.0.0.1:64571', name: 5, status: init, memory: 0, processing: 0>\n",
      "Starting worker compute stream, tcp://127.0.0.1:64571\n",
      "Starting established connection to tcp://127.0.0.1:64574\n",
      "Register worker <WorkerState 'tcp://127.0.0.1:64581', name: 0, status: init, memory: 0, processing: 0>\n",
      "Starting worker compute stream, tcp://127.0.0.1:64581\n",
      "Starting established connection to tcp://127.0.0.1:64583\n",
      "Register worker <WorkerState 'tcp://127.0.0.1:64584', name: 1, status: init, memory: 0, processing: 0>\n",
      "Starting worker compute stream, tcp://127.0.0.1:64584\n",
      "Starting established connection to tcp://127.0.0.1:64588\n",
      "Register worker <WorkerState 'tcp://127.0.0.1:64585', name: 2, status: init, memory: 0, processing: 0>\n",
      "Starting worker compute stream, tcp://127.0.0.1:64585\n",
      "Starting established connection to tcp://127.0.0.1:64589\n",
      "Receive client connection: Client-223c5ca9-2255-11ee-a69c-14857f4ff0a5\n",
      "Starting established connection to tcp://127.0.0.1:64590\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EntitySet scattered to 8 workers in 5 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Remove client Client-223c5ca9-2255-11ee-a69c-14857f4ff0a5\n",
      "Received 'close-stream' from tcp://127.0.0.1:64590; closing.\n",
      "Remove client Client-223c5ca9-2255-11ee-a69c-14857f4ff0a5\n",
      "Close client connection: Client-223c5ca9-2255-11ee-a69c-14857f4ff0a5\n",
      "Closing Nanny at 'tcp://127.0.0.1:64528'. Reason: nanny-close\n",
      "Nanny asking worker to close. Reason: nanny-close\n",
      "Closing Nanny at 'tcp://127.0.0.1:64529'. Reason: nanny-close\n",
      "Nanny asking worker to close. Reason: nanny-close\n",
      "Closing Nanny at 'tcp://127.0.0.1:64530'. Reason: nanny-close\n",
      "Nanny asking worker to close. Reason: nanny-close\n",
      "Closing Nanny at 'tcp://127.0.0.1:64531'. Reason: nanny-close\n",
      "Nanny asking worker to close. Reason: nanny-close\n",
      "Closing Nanny at 'tcp://127.0.0.1:64532'. Reason: nanny-close\n",
      "Nanny asking worker to close. Reason: nanny-close\n",
      "Closing Nanny at 'tcp://127.0.0.1:64533'. Reason: nanny-close\n",
      "Nanny asking worker to close. Reason: nanny-close\n",
      "Closing Nanny at 'tcp://127.0.0.1:64534'. Reason: nanny-close\n",
      "Nanny asking worker to close. Reason: nanny-close\n",
      "Closing Nanny at 'tcp://127.0.0.1:64535'. Reason: nanny-close\n",
      "Nanny asking worker to close. Reason: nanny-close\n",
      "Received 'close-stream' from tcp://127.0.0.1:64566; closing.\n",
      "Received 'close-stream' from tcp://127.0.0.1:64576; closing.\n",
      "Received 'close-stream' from tcp://127.0.0.1:64578; closing.\n",
      "Received 'close-stream' from tcp://127.0.0.1:64574; closing.\n",
      "Received 'close-stream' from tcp://127.0.0.1:64569; closing.\n",
      "Received 'close-stream' from tcp://127.0.0.1:64583; closing.\n",
      "Received 'close-stream' from tcp://127.0.0.1:64589; closing.\n",
      "Remove worker <WorkerState 'tcp://127.0.0.1:64564', name: 3, status: closing, memory: 0, processing: 0>\n",
      "Removing comms to tcp://127.0.0.1:64564\n",
      "Remove worker <WorkerState 'tcp://127.0.0.1:64570', name: 4, status: closing, memory: 0, processing: 0>\n",
      "Removing comms to tcp://127.0.0.1:64570\n",
      "Remove worker <WorkerState 'tcp://127.0.0.1:64572', name: 7, status: closing, memory: 0, processing: 0>\n",
      "Removing comms to tcp://127.0.0.1:64572\n",
      "Remove worker <WorkerState 'tcp://127.0.0.1:64571', name: 5, status: closing, memory: 0, processing: 0>\n",
      "Removing comms to tcp://127.0.0.1:64571\n",
      "Remove worker <WorkerState 'tcp://127.0.0.1:64567', name: 6, status: closing, memory: 0, processing: 0>\n",
      "Removing comms to tcp://127.0.0.1:64567\n",
      "Remove worker <WorkerState 'tcp://127.0.0.1:64581', name: 0, status: closing, memory: 0, processing: 0>\n",
      "Removing comms to tcp://127.0.0.1:64581\n",
      "Remove worker <WorkerState 'tcp://127.0.0.1:64585', name: 2, status: closing, memory: 0, processing: 0>\n",
      "Removing comms to tcp://127.0.0.1:64585\n",
      "Received 'close-stream' from tcp://127.0.0.1:64588; closing.\n",
      "Remove worker <WorkerState 'tcp://127.0.0.1:64584', name: 1, status: closing, memory: 1, processing: 0>\n",
      "Removing comms to tcp://127.0.0.1:64584\n",
      "Lost all workers\n",
      "Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:64523 remote=tcp://127.0.0.1:64588>\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\anaconda3\\envs\\myenv\\lib\\site-packages\\distributed\\batched.py\", line 115, in _background_send\n",
      "    nbytes = yield coro\n",
      "  File \"D:\\anaconda3\\envs\\myenv\\lib\\site-packages\\tornado\\gen.py\", line 769, in run\n",
      "    value = future.result()\n",
      "  File \"D:\\anaconda3\\envs\\myenv\\lib\\site-packages\\distributed\\comm\\tcp.py\", line 269, in write\n",
      "    raise CommClosedError()\n",
      "distributed.comm.core.CommClosedError\n",
      "Scheduler closing...\n",
      "Scheduler closing all comms\n"
     ]
    }
   ],
   "source": [
    "# 添加dataframe数据 \n",
    "es.add_dataframe(dataframe_name = 'bigmart', dataframe = Train, index = 'id')\n",
    "trans_primitives=['add_numeric', 'subtract_numeric', 'multiply_numeric', 'divide_numeric'] # 2列相加减乘除来生成新特征\n",
    "agg_primitives=['sum', 'median','mean']\n",
    "feature_matrix, feature_names = ft.dfs(entityset=es, \n",
    "                                       target_dataframe_name = 'bigmart', \n",
    "                                       max_depth = 2, \n",
    "                                       verbose = 0,\n",
    "                                       agg_primitives=agg_primitives,\n",
    "                                       trans_primitives=trans_primitives,\n",
    "                                       n_jobs = 8)\n",
    "es.add_dataframe(dataframe_name = 'bigmart_valid', dataframe = Valid, index = 'id')\n",
    "feature_matrix_valid, feature_names_valid = ft.dfs(entityset=es, \n",
    "                                       target_dataframe_name = 'bigmart_valid', \n",
    "                                       max_depth = 2, \n",
    "                                       verbose = 0,\n",
    "                                       agg_primitives=agg_primitives,\n",
    "                                       trans_primitives=trans_primitives,\n",
    "                                       n_jobs = 8)\n",
    "#     max_depth 控制由叠加特征基元方式生成的特征的复杂性。\n",
    "#     agg_primitives 是定义了一些统计聚合方式。\n",
    "#     trans_primitives 定义了变换计算算子。\n",
    "#     n_jobs 设定了多核并行特征计算的核数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c298bcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['57', '11', '58', '74', '75', '103', '83', '82', '86', '80',\n",
       "       ...\n",
       "       '94 - 95', '94 - 97', '94 - 98', '94 - 99', '95 - 97', '95 - 98',\n",
       "       '95 - 99', '97 - 98', '97 - 99', '98 - 99'],\n",
       "      dtype='object', length=24850)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_matrix.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1c31d44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['57', '11', '58', '74', '75', '103', '83', '82', '86', '80',\n",
       "       ...\n",
       "       '94 - 95', '94 - 97', '94 - 98', '94 - 99', '95 - 97', '95 - 98',\n",
       "       '95 - 99', '97 - 98', '97 - 99', '98 - 99'],\n",
       "      dtype='object', length=24850)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_matrix_valid.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c760dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(type(feature_matrix))\n",
    "print(type(feature_matrix_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ea387c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#再次处理\n",
    "#删除含有INF和NAN的列\n",
    "# 查找包含 INF 值的列\n",
    "inf_cols = feature_matrix.columns[feature_matrix.isin([np.inf]).any()]\n",
    "\n",
    "# 删除包含 INF 值的列\n",
    "feature_matrix = feature_matrix.drop(columns=inf_cols)\n",
    "\n",
    "# 查找包含 NAN 值的列\n",
    "nan_cols = feature_matrix.columns[feature_matrix.isna().any()]\n",
    "# 删除包含 NAN 值的列\n",
    "feature_matrix = feature_matrix.drop(columns=nan_cols)\n",
    "\n",
    "\n",
    "#进行低相关性滤波\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "feature_matrix_Var = feature_matrix.copy()\n",
    "\n",
    "#方差阈值为0.5\n",
    "sel = VarianceThreshold(threshold=0.5)\n",
    "\n",
    "# 对数据集进行拟合和转换\n",
    "feature_matrix_Var = sel.fit_transform(feature_matrix_Var)\n",
    "\n",
    "# 将数组转换回DataFrame，并使用原始特征的名称\n",
    "feature_matrix_Var = pd.DataFrame(feature_matrix_Var, columns=feature_matrix.columns[sel.get_support()])\n",
    "\n",
    "#进行高相关性滤波\n",
    "feature_matrix_Label = pd.concat([feature_matrix_Var,Label],axis = 1)\n",
    "\n",
    "# 计算相关系数矩阵\n",
    "corr_matrix = feature_matrix_Label.corr().abs()\n",
    "\n",
    "# 选择上三角矩阵\n",
    "upper = corr_matrix.where(pd.np.triu(pd.np.ones(corr_matrix.shape), k=1).astype(pd.np.bool))\n",
    "\n",
    "# 找到相关性大于0.85的特征列\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.5)]\n",
    "\n",
    "# 删除相关性高的特征\n",
    "feature_matrix_Label = feature_matrix_Label.drop(feature_matrix_Label[to_drop], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4bea81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#删除含有INF和NAN的列\n",
    "# 查找包含 INF 值的列\n",
    "inf_cols = feature_matrix_valid.columns[feature_matrix_valid.isin([np.inf]).any()]\n",
    "\n",
    "# 删除包含 INF 值的列\n",
    "feature_matrix_valid = feature_matrix_valid.drop(columns=inf_cols)\n",
    "\n",
    "# 查找包含 NAN 值的列\n",
    "nan_cols = feature_matrix_valid.columns[feature_matrix_valid.isna().any()]\n",
    "# 删除包含 NAN 值的列\n",
    "feature_matrix_valid = feature_matrix_valid.drop(columns=nan_cols)\n",
    "\n",
    "\n",
    "#对验证集进行低相关性滤波和高相关性剔除,只需要提取相应的列索引就可以了\n",
    "feature_matrix_valid_Var = feature_matrix_valid.copy()\n",
    "\n",
    "feature_matrix_valid_Var = feature_matrix_valid_Var.filter(items = feature_matrix_Var.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766d7b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix_Var.columns\n",
    "feature_matrix_valid_Var.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efc0d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#获取特征重要性\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "model = CatBoostClassifier(iterations = 500,verbose = 100)\n",
    "model.fit(feature_matrix_Var, Label)\n",
    "\n",
    "#获取特征重要性\n",
    "feature_importances = model.get_feature_importance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d44b5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_importances 是一个包含所有特征重要性的数组\n",
    "# 我们将创建一个布尔数组，其中重要性大于0.01的元素为True，其他元素为False\n",
    "important_features_mask = feature_importances > 0\n",
    "\n",
    "# 使用这个布尔数组来索引特征名称，得到所有重要性大于0的特征\n",
    "features_matrix_Var_importance = feature_matrix_Var.columns[important_features_mask]\n",
    "\n",
    "features_matrix_Var_importance = feature_matrix_Var.filter(items = features_matrix_Var_importance)\n",
    "features_matrix_valid_Var_importance = feature_matrix_valid_Var.filter(items = features_matrix_Var_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf8f74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(features_matrix_Var_importance.shape)\n",
    "print(features_matrix_valid_Var_importance.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fe261f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#检验一下\n",
    "model = CatBoostClassifier(#n_estimators = 500,\n",
    "                           #l2_leaf_reg = 0.4,\n",
    "                           auto_class_weights='Balanced',\n",
    "                           boosting_type = 'Ordered',\n",
    "                           verbose = 100)\n",
    "model.fit(features_matrix_Var_importance, Label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87df618",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_predict = model.predict(features_matrix_valid_Var_importance)\n",
    "valid_pred_proba = model.predict_proba(features_matrix_valid_Var_importance)\n",
    "report = classification_report(Label_Valid,valid_predict)\n",
    "report_dict = classification_report(Label_Valid, valid_predict,output_dict=True)\n",
    "\n",
    "print(report)\n",
    "print(\"MacroF1:{:.2f}%\".format(MacroF1(report_dict) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1898ea96",
   "metadata": {},
   "source": [
    "# 正式开始"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03937f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#获得融合了验证集和训练集的数据\n",
    "tain_vlid_filename = r\"preprocess_train.csv\"\n",
    "tain_valid = data_solve_RFECV_train_valid(100)\n",
    "\n",
    "Train = train.drop(['sample_id','label'],axis = 1)\n",
    "Label = train[['label']]\n",
    "\n",
    "Train_Valid = tain_valid.drop(['sample_id','label'],axis = 1)\n",
    "Label_Train_Valid = tain_valid[['label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38ea74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#进行特征衍生\n",
    "es.add_dataframe(dataframe_name = 'bigmart_valid', dataframe = Train_Valid, index = 'id')\n",
    "feature_matrix_Train_Valid, feature_names_Train_Valid = ft.dfs(entityset=es, \n",
    "                                       target_dataframe_name = 'bigmart_valid', \n",
    "                                       max_depth = 2, \n",
    "                                       verbose = 0,\n",
    "                                       agg_primitives=agg_primitives,\n",
    "                                       trans_primitives=trans_primitives,\n",
    "                                       n_jobs = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6603fbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用过滤器，直接进行特征选择即可\n",
    "\n",
    "feature_matrix_Train_Valid = feature_matrix_Train_Valid.filter(item = features_matrix_Var_importance.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a6da58",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(features_matrix_Train_Valid.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ec6865",
   "metadata": {},
   "outputs": [],
   "source": [
    "#选择特征\n",
    "feature_matrix_Train_Valid.columns == features_matrix_Var_importance.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5240a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#进行模型训练\n",
    "\n",
    "model = model = CatBoostClassifier(\n",
    "    auto_class_weights='Balanced',\n",
    "    boosting_type = 'Ordered',                               \n",
    "    verbose = 100\n",
    ")\n",
    "\n",
    "model.fit(Train_Valid,Label_Train_Valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce1a9c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#获得测试数据\n",
    "test =  data_solve_RFECV_test(r\"test_2000_x.csv\",100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "892a813b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 101)\n"
     ]
    }
   ],
   "source": [
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56668ca3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['sample_id',          57,          11,          58,          74,\n",
      "                75,         103,          83,          82,          86,\n",
      "       ...\n",
      "                35,          33,         105,          19,          46,\n",
      "                 5,          22,          71,          10,          15],\n",
      "      dtype='object', length=101)\n"
     ]
    }
   ],
   "source": [
    "print(test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "001ce9f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'es' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[7], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m#进行特征衍生\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m \u001B[43mes\u001B[49m\u001B[38;5;241m.\u001B[39madd_dataframe(dataframe_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbigmart_valid\u001B[39m\u001B[38;5;124m'\u001B[39m, dataframe \u001B[38;5;241m=\u001B[39m test, index \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mid\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m      3\u001B[0m feature_matrix_test, feature_names_test \u001B[38;5;241m=\u001B[39m ft\u001B[38;5;241m.\u001B[39mdfs(entityset\u001B[38;5;241m=\u001B[39mes, \n\u001B[0;32m      4\u001B[0m                                        target_dataframe_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbigmart_valid\u001B[39m\u001B[38;5;124m'\u001B[39m, \n\u001B[0;32m      5\u001B[0m                                        max_depth \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m2\u001B[39m, \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m      8\u001B[0m                                        trans_primitives\u001B[38;5;241m=\u001B[39mtrans_primitives,\n\u001B[0;32m      9\u001B[0m                                        n_jobs \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m8\u001B[39m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'es' is not defined"
     ]
    }
   ],
   "source": [
    "#进行特征衍生\n",
    "es.add_dataframe(dataframe_name = 'bigmart_valid', dataframe = test, index = 'id')\n",
    "feature_matrix_test, feature_names_test = ft.dfs(entityset=es, \n",
    "                                       target_dataframe_name = 'bigmart_valid', \n",
    "                                       max_depth = 2, \n",
    "                                       verbose = 0,\n",
    "                                       agg_primitives=agg_primitives,\n",
    "                                       trans_primitives=trans_primitives,\n",
    "                                       n_jobs = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d4a8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#选择特征\n",
    "feature_matrix_test = feature_matrix_test.filter(items = feature_matrix_Var.columns)\n",
    "#应该直接消除掉\"sample_id“列了\n",
    "x_test = test.drop(['sample_id'],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5457e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#进行最终结果的预测\n",
    "predict = model.predict(feature_matrix_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2b9e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#获得预测结果\n",
    "pred_count = []\n",
    "for i in range(6):\n",
    "    pred_count.append(np.count_nonzero(test_predict == i))\n",
    "    print(\"类别{}的数量：{}\".format(i,pred_count[i]))\n",
    "print(\"数据总数：{}\".format(np.size(test_predict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a543be8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#生成submit.json文件\n",
    "data_list = test_predict.tolist()\n",
    "data = {}\n",
    "for i, value in enumerate(data_list):\n",
    "    key = str(i)\n",
    "    value = data_list[i][0]\n",
    "    data[key] = value\n",
    "    \n",
    "# 将JSON字符串写入文件\n",
    "with open(\"submit.json\", \"w\") as file:\n",
    "    json.dump(data, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
